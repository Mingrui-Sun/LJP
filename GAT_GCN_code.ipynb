{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引入相关的库\n",
    "import json\n",
    "import codecs\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import codecs\n",
    "from tqdm import tqdm\n",
    "import jieba\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "import dgl\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from dgl.data import DGLDataset\n",
    "from dgl.nn.pytorch import GraphConv, GATConv\n",
    "from dgl.nn import AvgPooling, MaxPooling\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置源数据文件夹路径\n",
    "src_data_folder = './data'\n",
    "\n",
    "# 定义读取数据的函数get_data\n",
    "def get_data(file_path, flag=None): \n",
    "    \"\"\"读取数据\"\"\"\n",
    "    text = []\n",
    "    target = []\n",
    "    with codecs.open(file_path, 'r', encoding='utf-8') as fin:  \n",
    "        for line in fin:                                        \n",
    "            tmp_line = json.loads(line)\n",
    "            text.append(tmp_line[\"fact\"].strip())               \n",
    "            target.append(tmp_line[\"meta\"][\"accusation\"][0])    \n",
    "    return pd.DataFrame({'text': text, 'target': target})       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取数据...\n",
      "读取完成！\n"
     ]
    }
   ],
   "source": [
    "print('读取数据...')\n",
    "# 使用os.path.join构造完整的文件路径。\n",
    "# 调用get_data函数读取每个数据集，并将结果存储在变量train_df、valid_df和test_df中。\n",
    "train_df = get_data(os.path.join(src_data_folder, 'data_train.json'), flag='train')\n",
    "valid_df = get_data(os.path.join(src_data_folder, 'data_valid.json'), flag='valid')\n",
    "test_df = get_data(os.path.join(src_data_folder, 'data_test.json'), flag='test')\n",
    "\n",
    "print('读取完成！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((154592, 2), (17131, 2), (32508, 2))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看训练集、验证集和测试集各自的样本数量，你可以直接运行上述代码片段。这里，shape属性返回一个元组，表示DataFrame的维度（行数，列数）\n",
    "train_df.shape, valid_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>昌宁县人民检察院指控，2014年4月19日下午16时许，被告人段某驾拖车经过鸡飞乡澡塘街子，...</td>\n",
       "      <td>故意伤害</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>公诉机关指控,2015年11月10日晚9时许，被告人李某的妹妹李某某与被害人华某某在桦川县悦...</td>\n",
       "      <td>故意伤害</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>贵州省平坝县人民检察院指控：2014年4月9日下午，被告人王某丁与其堂哥王4某（另案处理）假...</td>\n",
       "      <td>故意伤害</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>经审理查明：2014年5月6日14时许，被告人叶某某驾车途径赤壁市赵李桥镇胜利街涵洞时，被在...</td>\n",
       "      <td>故意伤害</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>安阳县人民检察院指控：2014年4月27日上午11时许，宋某甲在安阳县吕村镇翟奇务村被告人梁...</td>\n",
       "      <td>故意伤害</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text target\n",
       "0  昌宁县人民检察院指控，2014年4月19日下午16时许，被告人段某驾拖车经过鸡飞乡澡塘街子，...   故意伤害\n",
       "1  公诉机关指控,2015年11月10日晚9时许，被告人李某的妹妹李某某与被害人华某某在桦川县悦...   故意伤害\n",
       "2  贵州省平坝县人民检察院指控：2014年4月9日下午，被告人王某丁与其堂哥王4某（另案处理）假...   故意伤害\n",
       "3  经审理查明：2014年5月6日14时许，被告人叶某某驾车途径赤壁市赵李桥镇胜利街涵洞时，被在...   故意伤害\n",
       "4  安阳县人民检察院指控：2014年4月27日上午11时许，宋某甲在安阳县吕村镇翟奇务村被告人梁...   故意伤害"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练集前五行\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>公诉机关起诉指控，被告人张某某秘密窃取他人财物，价值2210元，××数额较大，其行为已触犯《...</td>\n",
       "      <td>盗窃</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>孝昌县人民检察院指控：2014年1月4日，被告人邬某在孝昌县城区2路公交车上××被害人晏某白...</td>\n",
       "      <td>盗窃</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>广东省广州市南沙区人民检察院指控被告人罗某于2015年6月2日到广州市南沙区大岗镇人民路宇航...</td>\n",
       "      <td>盗窃</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>公诉机关指控，2016年3月3日18时许，被告人易某某行至达州市通川区大观园公交车站附近，扒...</td>\n",
       "      <td>盗窃</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>公诉机关指控：1.2015年8月20日晚上，被告人胡某甲至杭州市淳安县千岛湖镇新安东路112...</td>\n",
       "      <td>盗窃</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text target\n",
       "0  公诉机关起诉指控，被告人张某某秘密窃取他人财物，价值2210元，××数额较大，其行为已触犯《...     盗窃\n",
       "1  孝昌县人民检察院指控：2014年1月4日，被告人邬某在孝昌县城区2路公交车上××被害人晏某白...     盗窃\n",
       "2  广东省广州市南沙区人民检察院指控被告人罗某于2015年6月2日到广州市南沙区大岗镇人民路宇航...     盗窃\n",
       "3  公诉机关指控，2016年3月3日18时许，被告人易某某行至达州市通川区大观园公交车站附近，扒...     盗窃\n",
       "4  公诉机关指控：1.2015年8月20日晚上，被告人胡某甲至杭州市淳安县千岛湖镇新安东路112...     盗窃"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 验证集前五行\n",
    "valid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>公诉机关指控：2016年3月28日20时许，被告人颜某在本市洪山区马湖新村足球场马路边捡拾到...</td>\n",
       "      <td>盗窃</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>天津市静海县人民检察院指控，2014年5月13日上午8时许，被告人李xx在天津市静海县大邱庄...</td>\n",
       "      <td>盗窃</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>永顺县人民检察院指控，2014年1月11日，被告人李某某与彭某某（另案处理）在永顺县塔卧镇“...</td>\n",
       "      <td>强奸</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>公诉机关起诉书指控：2016年11月17日凌晨1时许，被告人周某在本县武康街道营盘小区131...</td>\n",
       "      <td>盗窃</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>大名县人民检察院起诉书指控，2014年3月25日9时许，被告人张某在自家庄某处因故与本村席某...</td>\n",
       "      <td>故意伤害</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text target\n",
       "0  公诉机关指控：2016年3月28日20时许，被告人颜某在本市洪山区马湖新村足球场马路边捡拾到...     盗窃\n",
       "1  天津市静海县人民检察院指控，2014年5月13日上午8时许，被告人李xx在天津市静海县大邱庄...     盗窃\n",
       "2  永顺县人民检察院指控，2014年1月11日，被告人李某某与彭某某（另案处理）在永顺县塔卧镇“...     强奸\n",
       "3  公诉机关起诉书指控：2016年11月17日凌晨1时许，被告人周某在本县武康街道营盘小区131...     盗窃\n",
       "4  大名县人民检察院起诉书指控，2014年3月25日9时许，被告人张某在自家庄某处因故与本村席某...   故意伤害"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试集前五行\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总标签数:  195\n"
     ]
    }
   ],
   "source": [
    "# 获取所有唯一标签\n",
    "labels = train_df['target'].unique()    \n",
    "print('总标签数: ', len(labels))\n",
    "\n",
    "# 创建标签到ID的映射\n",
    "label2idx = {l:i for i,l in enumerate(sorted(labels))}   \n",
    "idx2label = {v:k for k,v in label2idx.items()}           \n",
    "\n",
    "# 使用apply()方法和之前创建的label2idx字典，将三个数据集中'target'列的标签替换为其对应的整数ID。\n",
    "# 将标签转换为ID\n",
    "train_df['target'] = train_df['target'].apply(lambda x: label2idx[x])\n",
    "valid_df['target'] = valid_df['target'].apply(lambda x: label2idx[x])\n",
    "test_df['target'] = test_df['target'].apply(lambda x: label2idx[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>昌宁县人民检察院指控，2014年4月19日下午16时许，被告人段某驾拖车经过鸡飞乡澡塘街子，...</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>公诉机关指控,2015年11月10日晚9时许，被告人李某的妹妹李某某与被害人华某某在桦川县悦...</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>贵州省平坝县人民检察院指控：2014年4月9日下午，被告人王某丁与其堂哥王4某（另案处理）假...</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>经审理查明：2014年5月6日14时许，被告人叶某某驾车途径赤壁市赵李桥镇胜利街涵洞时，被在...</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>安阳县人民检察院指控：2014年4月27日上午11时许，宋某甲在安阳县吕村镇翟奇务村被告人梁...</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  昌宁县人民检察院指控，2014年4月19日下午16时许，被告人段某驾拖车经过鸡飞乡澡塘街子，...      95\n",
       "1  公诉机关指控,2015年11月10日晚9时许，被告人李某的妹妹李某某与被害人华某某在桦川县悦...      95\n",
       "2  贵州省平坝县人民检察院指控：2014年4月9日下午，被告人王某丁与其堂哥王4某（另案处理）假...      95\n",
       "3  经审理查明：2014年5月6日14时许，被告人叶某某驾车途径赤壁市赵李桥镇胜利街涵洞时，被在...      95\n",
       "4  安阳县人民检察院指控：2014年4月27日上午11时许，宋某甲在安阳县吕村镇翟奇务村被告人梁...      95"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查训练集前5条数据\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>公诉机关起诉指控，被告人张某某秘密窃取他人财物，价值2210元，××数额较大，其行为已触犯《...</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>孝昌县人民检察院指控：2014年1月4日，被告人邬某在孝昌县城区2路公交车上××被害人晏某白...</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>广东省广州市南沙区人民检察院指控被告人罗某于2015年6月2日到广州市南沙区大岗镇人民路宇航...</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>公诉机关指控，2016年3月3日18时许，被告人易某某行至达州市通川区大观园公交车站附近，扒...</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>公诉机关指控：1.2015年8月20日晚上，被告人胡某甲至杭州市淳安县千岛湖镇新安东路112...</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  公诉机关起诉指控，被告人张某某秘密窃取他人财物，价值2210元，××数额较大，其行为已触犯《...     108\n",
       "1  孝昌县人民检察院指控：2014年1月4日，被告人邬某在孝昌县城区2路公交车上××被害人晏某白...     108\n",
       "2  广东省广州市南沙区人民检察院指控被告人罗某于2015年6月2日到广州市南沙区大岗镇人民路宇航...     108\n",
       "3  公诉机关指控，2016年3月3日18时许，被告人易某某行至达州市通川区大观园公交车站附近，扒...     108\n",
       "4  公诉机关指控：1.2015年8月20日晚上，被告人胡某甲至杭州市淳安县千岛湖镇新安东路112...     108"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查验证集前5条数据\n",
    "valid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>公诉机关指控：2016年3月28日20时许，被告人颜某在本市洪山区马湖新村足球场马路边捡拾到...</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>天津市静海县人民检察院指控，2014年5月13日上午8时许，被告人李xx在天津市静海县大邱庄...</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>永顺县人民检察院指控，2014年1月11日，被告人李某某与彭某某（另案处理）在永顺县塔卧镇“...</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>公诉机关起诉书指控：2016年11月17日凌晨1时许，被告人周某在本县武康街道营盘小区131...</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>大名县人民检察院起诉书指控，2014年3月25日9时许，被告人张某在自家庄某处因故与本村席某...</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  公诉机关指控：2016年3月28日20时许，被告人颜某在本市洪山区马湖新村足球场马路边捡拾到...     108\n",
       "1  天津市静海县人民检察院指控，2014年5月13日上午8时许，被告人李xx在天津市静海县大邱庄...     108\n",
       "2  永顺县人民检察院指控，2014年1月11日，被告人李某某与彭某某（另案处理）在永顺县塔卧镇“...      70\n",
       "3  公诉机关起诉书指控：2016年11月17日凌晨1时许，被告人周某在本县武康街道营盘小区131...     108\n",
       "4  大名县人民检察院起诉书指控，2014年3月25日9时许，被告人张某在自家庄某处因故与本村席某...      95"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查测试集前5条数据\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>罪名</th>\n",
       "      <th>编号</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[伪造、倒卖]伪造的有价票证</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[伪造、变造]居民身份证</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[伪造、变造]金融票证</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[伪造、变造、买卖]国家机关[公文、证件、印章]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[伪造、变造、买卖]武装部队[公文、证件、印章]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>非法行医</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>非法进行节育手术</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>非法采矿</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>骗取[贷款、票据承兑、金融票证]</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>高利转贷</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>195 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           罪名   编号\n",
       "0              [伪造、倒卖]伪造的有价票证    0\n",
       "1                [伪造、变造]居民身份证    1\n",
       "2                 [伪造、变造]金融票证    2\n",
       "3    [伪造、变造、买卖]国家机关[公文、证件、印章]    3\n",
       "4    [伪造、变造、买卖]武装部队[公文、证件、印章]    4\n",
       "..                        ...  ...\n",
       "190                      非法行医  190\n",
       "191                  非法进行节育手术  191\n",
       "192                      非法采矿  192\n",
       "193          骗取[贷款、票据承兑、金融票证]  193\n",
       "194                      高利转贷  194\n",
       "\n",
       "[195 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 罪名与id之间的映射,从罪名到其对应编号的映射关系\n",
    "a=pd.DataFrame(list(label2idx.items()),columns=['罪名', '编号'])\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分词和去停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义停用词文件的路径STOPWORDS_PATH，这里设为./aux_files/stopwords\n",
    "STOPWORDS_PATH = './aux_files/stopwords'\n",
    "stop_words = []                         \n",
    "with codecs.open(STOPWORDS_PATH, 'r', encoding='utf-8') as fin:        \n",
    "    for line in fin:\n",
    "        stop_words.append(line.strip())             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对输入的文本进行分词处理，并去除停用词\n",
    "def clean_text(text, stop_words):          \n",
    "    \"\"\"分词，去停用词\"\"\"\n",
    "    cleaned = []\n",
    "    for w in jieba.lcut(text):\n",
    "        if len(w.strip()) > 0 and w not in stop_words:\n",
    "            cleaned.append(w)\n",
    "    return ' '.join(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分词和去停用词...\n",
      "新处理数据...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.673 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>昌宁县 人民检察院 指控 2014 年 月 19 日 下午 16 时许 被告人 段 某驾 拖...</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>公诉 机关 指控 2015 年 11 月 10 日晚 时许 被告人 李某 妹妹 李 被害人 ...</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>贵州省 平坝县 人民检察院 指控 2014 年 月 日 下午 被告人 王某 丁 堂哥 王 另...</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  昌宁县 人民检察院 指控 2014 年 月 19 日 下午 16 时许 被告人 段 某驾 拖...      95\n",
       "1  公诉 机关 指控 2015 年 11 月 10 日晚 时许 被告人 李某 妹妹 李 被害人 ...      95\n",
       "2  贵州省 平坝县 人民检察院 指控 2014 年 月 日 下午 被告人 王某 丁 堂哥 王 另...      95"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('分词和去停用词...')\n",
    "\n",
    "# 对训练集、验证集和测试集的文本数据进行分词和去除停用词的预处理，并将处理后的数据持久化存储，以便后续重复使用而无需再次执行耗时的预处理步骤。\n",
    "# 首先检查dat_pkl_path指定的文件是否存在，如果存在，则从该文件中加载之前保存的预处理数据集。\n",
    "dat_pkl_path = './aux_files/data.pkl'\n",
    "if os.path.exists(dat_pkl_path):\n",
    "    print('加载数据...')\n",
    "    with open(dat_pkl_path, 'rb') as fin:\n",
    "        train_df, valid_df, test_df = pickle.load(fin)\n",
    "else:\n",
    "    # 如果预处理数据不存在，那么对每个数据集的'text'列应用clean_text函数，进行分词和去除停用词的操作。\n",
    "    # 处理完成后，使用pickle模块将处理后的数据集序列化并保存到dat_pkl_path指定的文件中。\n",
    "    # 第一次处理过程较长。\n",
    "    print('新处理数据...')\n",
    "    train_df['text'] = train_df['text'].apply(lambda x: clean_text(x, stop_words))\n",
    "    valid_df['text'] = valid_df['text'].apply(lambda x: clean_text(x, stop_words))\n",
    "    test_df['text'] = test_df['text'].apply(lambda x: clean_text(x, stop_words))    \n",
    "    # 文件存储\n",
    "    with open(dat_pkl_path, 'wb') as fout:\n",
    "        pickle.dump([train_df, valid_df, test_df], fout)\n",
    "        \n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>昌宁县 人民检察院 指控 2014 年 月 19 日 下午 16 时许 被告人 段 某驾 拖...</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>公诉 机关 指控 2015 年 11 月 10 日晚 时许 被告人 李某 妹妹 李 被害人 ...</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>贵州省 平坝县 人民检察院 指控 2014 年 月 日 下午 被告人 王某 丁 堂哥 王 另...</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>审理 查明 2014 年 月 日 14 时许 被告人 叶 驾车 途径 赤壁市 赵李桥镇 胜利...</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>安阳县 人民检察院 指控 2014 年 月 27 日 上午 11 时许 宋某 甲 安阳县 吕...</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  昌宁县 人民检察院 指控 2014 年 月 19 日 下午 16 时许 被告人 段 某驾 拖...      95\n",
       "1  公诉 机关 指控 2015 年 11 月 10 日晚 时许 被告人 李某 妹妹 李 被害人 ...      95\n",
       "2  贵州省 平坝县 人民检察院 指控 2014 年 月 日 下午 被告人 王某 丁 堂哥 王 另...      95\n",
       "3  审理 查明 2014 年 月 日 14 时许 被告人 叶 驾车 途径 赤壁市 赵李桥镇 胜利...      95\n",
       "4  安阳县 人民检察院 指控 2014 年 月 27 日 上午 11 时许 宋某 甲 安阳县 吕...      95"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>公诉 机关 起诉 指控 被告人 张 秘密 窃取 人财物 价值 2210 元 数额较大 已触犯...</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>孝昌县 人民检察院 指控 2014 年 月 日 被告人 邬某 孝昌县 城区 路 公交车 被害...</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>广东省 广州市 南沙 区 人民检察院 指控 被告人 罗某 2015 年 月 日到 广州市 南...</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>公诉 机关 指控 2016 年 月 日 18 时许 被告人 易 行至 达州市 通川区 大观园...</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>公诉 机关 指控 1.2015 年 月 20 日 晚上 被告人 胡某 甲 杭州市 淳安县 千...</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  公诉 机关 起诉 指控 被告人 张 秘密 窃取 人财物 价值 2210 元 数额较大 已触犯...     108\n",
       "1  孝昌县 人民检察院 指控 2014 年 月 日 被告人 邬某 孝昌县 城区 路 公交车 被害...     108\n",
       "2  广东省 广州市 南沙 区 人民检察院 指控 被告人 罗某 2015 年 月 日到 广州市 南...     108\n",
       "3  公诉 机关 指控 2016 年 月 日 18 时许 被告人 易 行至 达州市 通川区 大观园...     108\n",
       "4  公诉 机关 指控 1.2015 年 月 20 日 晚上 被告人 胡某 甲 杭州市 淳安县 千...     108"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>公诉 机关 指控 2016 年 月 28 日 20 时许 被告人 颜某 本市 洪山区 马湖 ...</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>天津市 静海县 人民检察院 指控 2014 年 月 13 日 上午 时许 被告人 李 xx ...</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>永顺县 人民检察院 指控 2014 年 月 11 日 被告人 李 彭 另案处理 永顺县 塔卧...</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>公诉 机关 起诉书 指控 2016 年 11 月 17 日 凌晨 时许 被告人 周某 本县 ...</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>大名县 人民检察院 起诉书 指控 2014 年 月 25 日 时许 被告人 张某 庄 某处 ...</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  公诉 机关 指控 2016 年 月 28 日 20 时许 被告人 颜某 本市 洪山区 马湖 ...     108\n",
       "1  天津市 静海县 人民检察院 指控 2014 年 月 13 日 上午 时许 被告人 李 xx ...     108\n",
       "2  永顺县 人民检察院 指控 2014 年 月 11 日 被告人 李 彭 另案处理 永顺县 塔卧...      70\n",
       "3  公诉 机关 起诉书 指控 2016 年 11 月 17 日 凌晨 时许 被告人 周某 本县 ...     108\n",
       "4  大名县 人民检察院 起诉书 指控 2014 年 月 25 日 时许 被告人 张某 庄 某处 ...      95"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个函数train_word_embeddings，用于训练Word2Vec词嵌入模型。\n",
    "# Word2Vec是一种常用的词向量模型，能够将词汇映射到连续的向量空间中，从而捕捉词汇间的语义和语法关系。\n",
    "\n",
    "def train_word_embeddings(src_data_list, word_embedding_path):\n",
    "    # 在gensim库的新版本中，size参数已经被替换为vector_size,iter参数也被改为了epochs\n",
    "    model = Word2Vec(min_count=1, vector_size=100, window=5, sg=1, negative=5, sample=0.001, epochs=10, workers=16)  # 初始化Word2Vec模型 \n",
    "\n",
    "    # 使用src_data_list中的句子构建词汇表。src_data_list应该是已经分词后的句子列表\n",
    "    model.build_vocab(corpus_iterable=src_data_list) \n",
    "\n",
    "    # 模型训练。继续使用src_data_list训练模型，total_examples和epochs参数用于控制训练过程\n",
    "    model.train(corpus_iterable=src_data_list, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "    # 使用save_word2vec_format方法将词向量保存到指定的word_embedding_path文件中，这通常是.txt或.vec格式的文本文件\n",
    "    model.wv.save_word2vec_format(word_embedding_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词向量...\n",
      "len of src_data_list: 204231\n",
      "['审理', '查明', '2013', '年', '月', '日', '下午', '被告人', '田', '某甲', '大城县', '人民检察院', '住宅小区', '院内', '被害人', '乙', '言语', '不合', '厮打', '田', '某甲', '殴打', '乙', '致于', '某乙腰', '处横突', '骨折', '轻伤', '案发后', '田', '某甲', '赔偿', '乙', '经济损失', '乙', '谅解', '2013', '年', '月', '日', '被告人', '田', '某甲', '自动', '公安机关', '投案', '如实', '供认', '被害人', '乙', '事实', '上述事实', '被告人', '田', '某甲', '开庭审理', '过程', '中', '无异议', '被害人', '乙', '陈述', '证人', '某甲', '田某', '乙', '证言', '法医学', '人体', '损伤', '程度', '鉴定书', '协议书', '被告人', '田', '某甲', '投案', '时', '接受', '讯问', '笔录', '证据', '证实', '足以认定', '被告人', '田', '某甲', '居住', '村民', '委员会', '证明', '田', '某甲', '平时', '表现', '无前科', '劣迹', '判处', '本村', '不良影响']\n",
      "训练词向量...\n",
      "词向量训练完毕...\n"
     ]
    }
   ],
   "source": [
    "print('词向量...')\n",
    "# 首先将训练集、验证集和测试集的文本数据转换为单词列表，然后合并这些列表以准备训练词向量模型。\n",
    "# 接下来，它检查词向量文件是否存在，如果存在则直接加载，否则训练一个新的词向量模型并保存。\n",
    "word_embedding_path = './aux_files/own.word2vec'\n",
    "\n",
    "# 将文本数据转换为单词列表\n",
    "# 使用split()方法将每条文本数据分割成单词列表，然后将这些列表转换为Python的list类型。\n",
    "train_list = list(train_df['text'].apply(lambda x: x.split()))   \n",
    "valid_list = list(valid_df['text'].apply(lambda x: x.split()))\n",
    "test_list = list(test_df['text'].apply(lambda x: x.split()))\n",
    "\n",
    "# 合并所有单词列表\n",
    "# 将三个数据集的单词列表合并为一个大的列表src_data_list，用于词向量模型的训练。\n",
    "src_data_list = train_list + valid_list + test_list\n",
    "\n",
    "# 样本个数\n",
    "print(f'len of src_data_list: {len(src_data_list)}')\n",
    "# 展示前100个样本\n",
    "print(src_data_list[100])\n",
    "    \n",
    "if os.path.exists(word_embedding_path):   \n",
    "    print('读取词向量...')\n",
    "    word_embeddings = KeyedVectors.load_word2vec_format(word_embedding_path, binary=False)\n",
    "else:                                      # 如果词向量文件不存在，则训练新的词向量模型\n",
    "    print('训练词向量...')\n",
    "    train_word_embeddings(src_data_list, word_embedding_path)  \n",
    "    word_embeddings = KeyedVectors.load_word2vec_format(word_embedding_path, binary=False)\n",
    "    # 再次加载词向量，这次是为了确保模型已经训练并可以使用\n",
    "    print('词向量训练完毕...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.35918403, -0.04667911,  0.25713038,  0.40582597, -0.24120483,\n",
       "       -0.02736917,  0.13773754, -0.17984635, -0.08682428,  1.0998052 ,\n",
       "        0.18352605, -0.56912655, -0.9076452 ,  1.4508265 , -0.30117694,\n",
       "       -0.30532724, -0.58267844, -0.21307744,  0.8312608 , -0.14654765,\n",
       "        0.70851016, -0.56026274, -0.3424588 , -0.6085302 , -0.13092417,\n",
       "        0.44015393, -0.45098317, -0.5192443 , -0.43278164,  0.02986295,\n",
       "        0.11785586,  0.519084  ,  0.42937672,  0.09964062,  0.40245634,\n",
       "        0.8120307 ,  0.40212658, -0.04235575,  0.46651635, -0.9146508 ,\n",
       "       -0.1140155 ,  0.04243552,  0.22194055, -0.51430064, -0.1674125 ,\n",
       "        1.268741  , -0.24753623, -0.21854803,  0.05382195, -0.45756075,\n",
       "        0.4305988 ,  0.2658599 ,  0.10162612, -0.07577621, -0.33632684,\n",
       "       -0.4887352 ,  0.05610463, -0.02146637,  0.324482  , -0.5519011 ,\n",
       "       -0.27342287, -0.08804538,  0.93529624, -0.39524287,  0.5908051 ,\n",
       "        0.14378726,  0.09487395,  0.40591815, -0.24234499,  0.12931657,\n",
       "       -0.24010551, -0.09852201, -0.01094551, -0.72973096,  0.6581018 ,\n",
       "        0.3641653 ,  0.39333212,  0.65821606, -0.3345971 ,  0.4324103 ,\n",
       "        0.12897511, -0.38092384, -0.1473784 ,  0.23294455,  0.11447071,\n",
       "       -0.3679392 , -0.1820167 , -0.13189358, -0.37735555,  0.4114335 ,\n",
       "        0.25741094,  0.5255754 ,  0.37315294,  0.57894117, -0.4118989 ,\n",
       "        0.56944776, -1.3153182 ,  0.8536185 , -0.8756288 ,  0.49074158],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings['审理']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(doc_words_list, weighted_graph = True):   \n",
    "    \"\"\"构建图\"\"\"\n",
    "    \n",
    "    x_adj = []   # 邻接矩阵\n",
    "    x_feature = []   # 特征向量\n",
    "    doc_len_list = []\n",
    "    vocab_set = set()\n",
    "\n",
    "    # doc_words是一个样本\n",
    "    for doc_words in doc_words_list:\n",
    "        \n",
    "        # 样本长度\n",
    "        doc_len = len(doc_words)\n",
    "\n",
    "        # 使用内置函数set()对样本单词去重\n",
    "        doc_vocab = list(set(doc_words))\n",
    "        \n",
    "        # 每个单词是一个node\n",
    "        doc_nodes = len(doc_vocab)\n",
    "\n",
    "        doc_len_list.append(doc_nodes)\n",
    "        vocab_set.update(doc_vocab)\n",
    "\n",
    "        # word to id\n",
    "        doc_word_id_map = {}\n",
    "        for j in range(doc_nodes):\n",
    "            doc_word_id_map[doc_vocab[j]] = j\n",
    "\n",
    "        # 滑动窗口\n",
    "        windows = []\n",
    "        if doc_len <= window_size:\n",
    "            windows.append(doc_words)\n",
    "        else:\n",
    "            for j in range(doc_len - window_size + 1):\n",
    "                window = doc_words[j: j + window_size]\n",
    "                windows.append(window)\n",
    "\n",
    "        word_pair_count = {}\n",
    "        for window in windows:\n",
    "            for p in range(1, len(window)):\n",
    "                for q in range(0, p):\n",
    "                    word_p = window[p]\n",
    "                    word_p_id = word_id_map[word_p]\n",
    "                    word_q = window[q]\n",
    "                    word_q_id = word_id_map[word_q]\n",
    "                    if word_p_id == word_q_id:\n",
    "                        continue\n",
    "                    word_pair_key = (word_p_id, word_q_id)\n",
    "                    # 单词之间的共现作为权值\n",
    "                    if word_pair_key in word_pair_count:\n",
    "                        word_pair_count[word_pair_key] += 1.\n",
    "                    else:\n",
    "                        word_pair_count[word_pair_key] = 1.\n",
    "                    # 双向\n",
    "                    word_pair_key = (word_q_id, word_p_id)\n",
    "                    if word_pair_key in word_pair_count:\n",
    "                        word_pair_count[word_pair_key] += 1.\n",
    "                    else:\n",
    "                        word_pair_count[word_pair_key] = 1.\n",
    "    \n",
    "        row = []\n",
    "        col = []\n",
    "        weight = []\n",
    "        features = []\n",
    "\n",
    "        for key in word_pair_count:\n",
    "            p = key[0]\n",
    "            q = key[1]\n",
    "            row.append(doc_word_id_map[vocab[p]])\n",
    "            col.append(doc_word_id_map[vocab[q]])\n",
    "            weight.append(word_pair_count[key] if weighted_graph else 1.)\n",
    "        adj = sp.csr_matrix((weight, (row, col)), shape=(doc_nodes, doc_nodes))\n",
    "    \n",
    "        # for k, v in sorted(doc_word_id_map.items(), key=lambda x: x[1]):\n",
    "        #     features.append(word_embeddings.wv[k] if k in word_embeddings.vocab else oov[k])\n",
    "\n",
    "        for k, v in sorted(doc_word_id_map.items(), key=lambda x: x[1]):   \n",
    "            if k in word_embeddings.key_to_index:                           # 使用.key_to_index来检查词是否在词汇表中\n",
    "                features.append(word_embeddings[k])  # 直接使用key访问词向量\n",
    "            else:\n",
    "                features.append(oov[k])\n",
    "\n",
    "\n",
    "        x_adj.append(adj)\n",
    "        x_feature.append(features)\n",
    "\n",
    "    \n",
    "    return x_adj, x_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    # 最大遍历次数,表示模型在整个训练数据集上的迭代次数。更多的遍历次数通常意味着模型有更多机会学习数据中的模式，但也可能导致过拟合。\n",
    "    max_epochs = 30\n",
    "    # 学习率,是梯度下降算法中的关键参数，决定了权重更新的步长。较高的学习率可以使模型更快收敛，但可能导致训练不稳定；较低的学习率则可能使训练过程缓慢。\n",
    "    lr = 1e-3\n",
    "    # 批量大小,批量大小，表示每次更新模型权重时使用的样本数量。较大的批量大小可以加速训练，但可能需要更多的内存；较小的批量大小则可能有助于模型更好地泛化。\n",
    "    batch_size = 64\n",
    "    # 词向量维度,词向量维度，表示词嵌入向量的长度。较大的维度可以捕获更多的语义信息，但也会增加模型的复杂度和计算成本。\n",
    "    embedding_dim = 100\n",
    "    # GCN隐层神经元数,GCN隐层神经元数，表示图卷积网络中隐层的宽度。这直接影响模型的表达能力和计算复杂度。\n",
    "    hidden_dim = 128\n",
    "    # GAT的head参数,GAT的头参数，表示在图注意力网络中并行使用的注意力头的数量。多个头允许模型关注输入的不同部分，从而增强模型的表达能力。\n",
    "    num_heads = 8\n",
    "    # 滑动窗口大小,滑动窗口大小，用于确定构建图结构时考虑的上下文范围。较大的窗口可以捕获更长距离的依赖关系，但也可能引入噪音。\n",
    "    window_size = 3\n",
    "    # 模型保存路径，指定了训练好的模型将被保存的位置。这对于后续的模型复用和部署非常重要。\n",
    "    model_save_path = './model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "构建 training 图...\n",
      "训练图构建完成，耗时：165.38 秒\n",
      "构建 valid 图...\n",
      "训练图构建完成，耗时：16.55 秒\n",
      "构建 test 图...\n",
      "训练图构建完成，耗时：31.86 秒\n"
     ]
    }
   ],
   "source": [
    "import time    # 为了计算build_graph函数的执行时间\n",
    "\n",
    "# 1、构建词汇表\n",
    "\n",
    "word_embeddings_dim = args.embedding_dim\n",
    "\n",
    "# 初始化一个空集合word_set，用于存储所有文档中出现的唯一单词。\n",
    "word_set = set() \n",
    "\n",
    "# 遍历所有样本\n",
    "# 遍历src_data_list（假设这是包含所有文档的列表），更新word_set以包含所有文档中的单词\n",
    "for doc_words in src_data_list:\n",
    "    word_set.update(doc_words)\n",
    "\n",
    "# 将word_set转换为列表vocab，并计算其大小vocab_size\n",
    "vocab = list(word_set)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# 2、单词到ID的映射，word to id\n",
    "word_id_map = {}   \n",
    "# 通过遍历vocab列表，为每个单词分配一个从0开始的连续整数ID\n",
    "for i in range(vocab_size):\n",
    "    word_id_map[vocab[i]] = i\n",
    "\n",
    "# 3、初始化OOV（Out-Of-Vocabulary）词向量\n",
    "# 设置低频词典(oov)，创建一个字典oov，用于存储不在预训练词向量模型中的单词的随机初始化向量\n",
    "oov = {}\n",
    "# 对于vocab中的每个单词，生成一个维度为word_embeddings_dim的随机向量，范围在-0.1到0.1之间\n",
    "for v in vocab:\n",
    "    oov[v] = np.random.uniform(-0.1, 0.1, word_embeddings_dim)\n",
    "\n",
    "\n",
    "# 构建图结构    \n",
    "window_size = args.window_size\n",
    "\n",
    "# build_graph函数调用前后添加了计时代码。start_time记录函数调用前的时间戳，end_time记录函数调用完成后的时刻，两者之差即为函数的执行时间。\n",
    "start_time = time.time()\n",
    "print('构建 training 图...')\n",
    "train_x_adj, train_x_feature = build_graph(train_list, weighted_graph = True)\n",
    "end_time = time.time()\n",
    "assert len(train_x_adj) == len(train_x_feature) == len(train_list)\n",
    "print(f\"训练图构建完成，耗时：{end_time - start_time:.2f} 秒\")\n",
    "\n",
    "start_time = time.time()\n",
    "print('构建 valid 图...')\n",
    "valid_x_adj, valid_x_feature = build_graph(valid_list, weighted_graph = True)\n",
    "end_time = time.time()\n",
    "assert len(valid_x_adj) == len(valid_x_feature) == len(valid_list)\n",
    "print(f\"训练图构建完成，耗时：{end_time - start_time:.2f} 秒\")\n",
    "\n",
    "start_time = time.time()\n",
    "print('构建 test 图...')\n",
    "test_x_adj, test_x_feature = build_graph(test_list, weighted_graph = True)\n",
    "end_time = time.time()\n",
    "assert len(test_x_adj) == len(test_x_feature) == len(test_list)\n",
    "print(f\"训练图构建完成，耗时：{end_time - start_time:.2f} 秒\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备训练\n",
    "准备图数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GraphDataset类继承自DGLDataset，这是DGL（Deep Graph Library）框架中的一个基类，用于处理图数据集。\n",
    "# 这个类主要用于封装图数据及其对应的标签，以便于在PyTorch或其它深度学习框架中使用。\n",
    "class GraphDataset(DGLDataset):\n",
    "    \"\"\"图数据集\"\"\"    \n",
    "\n",
    "    # 1、初始化函数\n",
    "    def __init__(self, x_adj, x_feature, targets = None):\n",
    "        self.adj_matrix = x_adj\n",
    "        self.node_matrix = x_feature\n",
    "        self.targets = targets\n",
    "\n",
    "    # 2、长度函数：\n",
    "    def __len__(self):\n",
    "        return len(self.adj_matrix)\n",
    "\n",
    "    # 3、获取项函数：\n",
    "    def __getitem__(self, idx):\n",
    "        scipy_adj = self.adj_matrix[idx]\n",
    "        G = dgl.from_scipy(scipy_adj)\n",
    "        G.ndata['feat'] = torch.stack([torch.tensor(x, dtype = torch.float) for x in self.node_matrix[idx]])\n",
    "        if self.targets is not None:\n",
    "            label = self.targets[idx]\n",
    "            return G, torch.tensor(label, dtype = torch.long)\n",
    "        return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCNClassifier类是一个基于图卷积网络（GCN）的分类器，继承自PyTorch的nn.Module。这个类实现了两层GCN卷积，随后是平均池化、Dropout和全连接层，用于最终的分类任务。\n",
    "class GCNClassifier(nn.Module):\n",
    "    \"\"\"GCN\"\"\"    \n",
    "    # 初始化函数：\n",
    "    def __init__(self, in_dim, hidden_dim, n_classes):\n",
    "        super(GCNClassifier, self).__init__()\n",
    "        self.conv1 = GraphConv(in_dim, hidden_dim)\n",
    "        self.conv2 = GraphConv(hidden_dim, hidden_dim)\n",
    "        self.avgpooling = AvgPooling()\n",
    "        self.drop = nn.Dropout(p = 0.3)\n",
    "        self.classify = nn.Linear(hidden_dim, n_classes)\n",
    "\n",
    "    # 前向传播函数：\n",
    "    def forward(self, g, h):\n",
    "        h = F.relu(self.conv1(g, h))\n",
    "        h = F.relu(self.conv2(g, h))\n",
    "        h = self.drop(h)\n",
    "        h = self.avgpooling(g, h)\n",
    "        return self.classify(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GATClassifier类是一个基于图注意力网络（GAT）的分类器，同样继承自PyTorch的nn.Module。\n",
    "# 这个类通过多头注意力机制来增强图卷积的效果，适用于处理图结构数据的分类任务。\n",
    "class GATClassifier(nn.Module):\n",
    "    \"\"\"GAT\"\"\"    \n",
    "    # 初始化函数：\n",
    "    def __init__(self, in_dim, hidden_dim, num_heads, n_classes):\n",
    "        super(GATClassifier, self).__init__()\n",
    "        self.hid_dim = hidden_dim\n",
    "        self.gat1 = GATConv(in_dim, hidden_dim, num_heads)\n",
    "        self.gat2 = GATConv(hidden_dim*num_heads, hidden_dim, 1)\n",
    "        self.avgpooling = AvgPooling()\n",
    "        self.drop = nn.Dropout(p = 0.3)\n",
    "        self.classify = nn.Linear(hidden_dim, n_classes)\n",
    "\n",
    "    # 前向传播函数：\n",
    "    def forward(self, g, h):\n",
    "        # batch size批量大小\n",
    "        bs = h.shape[0]\n",
    "        h = F.relu(self.gat1(g, h))\n",
    "        h = h.reshape(bs, -1)\n",
    "        h = F.relu(self.gat2(g, h))\n",
    "        h = h.reshape(bs, -1)\n",
    "        h = self.drop(h)\n",
    "        h = self.avgpooling(g, h)\n",
    "        return self.classify(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这段代码定义了一个训练流程，用于训练图神经网络模型，如GCN或GAT\n",
    "def train(args, model, train_info, val_info, num_classes, model_type):\n",
    "    \"\"\"训练函数\"\"\"\n",
    "\n",
    "    # 1、数据准备\n",
    "    # 从train_info和val_info中提取训练集和验证集的邻接矩阵、节点特征和标签列表。\n",
    "    train_adj_list, train_node_list, train_label_list = train_info\n",
    "    val_adj_list, val_node_list, val_label_list = val_info\n",
    "\n",
    "    # 创建GraphDataset实例，分别封装训练集和验证集的数据\n",
    "    traindataset = GraphDataset(train_adj_list, train_node_list, train_label_list)\n",
    "    valdataset = GraphDataset(val_adj_list, val_node_list, val_label_list)\n",
    "    \n",
    "    # shuffle = True：在一个epoch之后，对所有的数据随机打乱，再按照设定好的每个批次的大小划分批次\n",
    "    trainloader = GraphDataLoader(traindataset, batch_size = args.batch_size, shuffle = True)  \n",
    "    valloader = GraphDataLoader(valdataset, batch_size = args.batch_size, shuffle = False)\n",
    "\n",
    "    # 2、定义损失函数和优化器\n",
    "    criterion = CrossEntropyLoss()   # 交叉熵损失函数函数\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = args.lr)    # 优化器\n",
    "\n",
    "    # 训练循环\n",
    "    best_val_f1 = 0\n",
    "\n",
    "    # 遍历args.max_epochs指定的epoch数\n",
    "    for idx in range(args.max_epochs):\n",
    "        print(f'Epoch {idx + 1}/{args.max_epochs}')\n",
    "\n",
    "        # 每个epoch中，先调用train_one_epoch函数进行模型训练，返回训练损失、准确率、F1分数和AUC值\n",
    "        train_loss, train_acc, train_f1, train_auc = train_one_epoch(trainloader, model, criterion, optimizer, num_classes)\n",
    "        \n",
    "        # 然后调用validate函数在验证集上评估模型性能，返回验证损失、准确率、F1分数和AUC值。\n",
    "        val_loss, val_acc, val_f1, val_auc = validate(valloader, model, criterion, num_classes)\n",
    "        \n",
    "        # 打印每个epoch的训练和验证指标\n",
    "        print('train_loss: {}, train_f1: {}, train_auc: {}, train_acc: {}, val_loss: {}, val_f1: {}, val_auc: {}, val_acc: {}'.\n",
    "              format(train_loss, train_f1, train_auc, train_acc, val_loss, val_f1, val_auc, val_acc))\n",
    "        \n",
    "        # 保存f1值最好的epoch的模型用于测试\n",
    "        # 如果当前验证集的F1分数高于历史最佳，保存当前模型状态到磁盘，以备后续测试使用\n",
    "        if val_f1 > best_val_f1:\n",
    "            print(f'----- Save model ----- with f1: {val_f1}')\n",
    "            torch.save(model.state_dict(), f'{args.model_save_path}/{model_type}-epoch-{idx}-{val_f1}.pt')\n",
    "            best_val_f1 = val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这段代码定义了train_one_epoch函数，用于执行模型在一个epoch上的训练过程。\n",
    "def train_one_epoch(trainloader, model, criterion, optimizer, num_classes):\n",
    "    \"\"\"训练一个 epoch\"\"\"\n",
    "\n",
    "# 1、初始化变量：\n",
    "    # 初始化train_loss、train_f1和train_auc变量，用于累计整个epoch的训练损失、F1分数和AUC值。\n",
    "    train_loss = 0\n",
    "    train_f1 = 0\n",
    "    train_auc = 0\n",
    "    # 创建两个空列表all_labels和all_logits，用于存储所有批次的真实标签和模型预测的logits。\n",
    "    all_labels = []\n",
    "    all_logits = []\n",
    "\n",
    "# 2、模型训练\n",
    "    total = len(trainloader)\n",
    "    model.train()\n",
    "    for idx, (G, label) in tqdm(enumerate(trainloader), total = total):\n",
    "                \n",
    "        h = G.ndata['feat'].float()\n",
    "        logit = model(G, h)\n",
    "        loss = criterion(logit, label)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()   # 反向传播梯度更新\n",
    "        \n",
    "        label_numpy = label.detach().cpu().numpy()\n",
    "        logit_numpy = logit.softmax(-1).detach().cpu().numpy()\n",
    "        \n",
    "        train_loss += loss.item()/total\n",
    "        # train_f1 += f1_score(label_numpy, logit_numpy.argmax(-1), average = 'micro')/total\n",
    "        \n",
    "        all_labels.append(label_numpy)\n",
    "        all_logits.append(logit_numpy)\n",
    "\n",
    "# 3、性能评估\n",
    "    # 在每个批次结束后，收集所有批次的真实标签和预测logits。\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_logits = np.concatenate(all_logits)\n",
    "\n",
    "    # 计算整个epoch的AUC值，使用roc_auc_score函数，参数multi_class='ovo'表示采用一对多策略计算多分类问题的AUC。\n",
    "    train_auc = roc_auc_score(all_labels, all_logits, multi_class = 'ovo', labels = np.array([int(i) for i in range(num_classes)]))\n",
    "\n",
    "     # 计算准确率和加权平均F1分数，使用accuracy_score和f1_score函数，其中f1_score的average='weighted'参数表示计算加权平均F1分数，权重由每个类别的样本数量决定。\n",
    "    all_logits_label = np.argmax(all_logits, 1)\n",
    "    train_acc = accuracy_score(all_labels, all_logits_label)\n",
    "    train_f1 = f1_score(all_labels, all_logits_label, average = 'weighted')\n",
    "\n",
    "# 4、返回结果\n",
    "    # 返回整个epoch的平均训练损失、准确率、F1分数和AUC值。 \n",
    "    return train_loss, train_acc, train_f1, train_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这段代码定义了validate函数，用于在验证集上评估模型的性能\n",
    "def validate(valloader, model, criterion, num_classes):\n",
    "    \"\"\"在验证集上验证\"\"\"\n",
    "\n",
    "# 1、初始化变量：\n",
    "    # 初始化val_loss、val_f1和val_auc变量，用于累计整个验证集的损失、F1分数和AUC值。\n",
    "    # 创建两个空列表all_labels和all_logits，用于存储所有批次的真实标签和模型预测的logits。\n",
    "    val_loss = 0\n",
    "    val_f1 = 0\n",
    "    val_auc = 0    \n",
    "    all_labels = []\n",
    "    all_logits = []\n",
    "\n",
    "# 2、模型评估：\n",
    "    total = len(valloader)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (G, label) in tqdm(enumerate(valloader), total = total):\n",
    "\n",
    "            h = G.ndata['feat'].float()\n",
    "            logit = model(G, h)\n",
    "            loss = criterion(logit, label)\n",
    "\n",
    "            label_numpy = label.detach().cpu().numpy()\n",
    "            logit_numpy = logit.softmax(-1).detach().cpu().numpy()\n",
    "\n",
    "            val_loss += loss.item()/total\n",
    "            # val_f1 += f1_score(label_numpy, logit_numpy.argmax(-1), average = 'micro')/total\n",
    "\n",
    "        \n",
    "            all_labels.append(label_numpy)\n",
    "            all_logits.append(logit_numpy)\n",
    "\n",
    "        all_labels = np.concatenate(all_labels)\n",
    "        all_logits = np.concatenate(all_logits)\n",
    "\n",
    "# 3、性能评估：\n",
    "    # 计算整个验证集的AUC值，使用roc_auc_score函数，参数multi_class='ovo'表示采用一对多策略计算多分类问题的AUC。           \n",
    "        val_auc = roc_auc_score(all_labels, all_logits, multi_class = 'ovo', labels = np.array([int(i) for i in range(num_classes)]))\n",
    "        all_logits_label = np.argmax(all_logits, 1)\n",
    "        \n",
    "    # 计算准确率和微平均F1分数，使用accuracy_score和f1_score函数，其中f1_score的average='micro'参数表示计算微平均F1分数，这适用于不平衡数据集的情况。 \n",
    "        val_acc = accuracy_score(all_labels, all_logits_label)\n",
    "        val_f1 = f1_score(all_labels, all_logits_label, average = 'micro')\n",
    "    \n",
    "    # 返回整个验证集的平均损失、准确率、F1分数和AUC值。\n",
    "    return val_loss, val_acc, val_f1, val_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAT 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_kg_hide-output": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- GAT 模型训练\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [03:01<00:00, 13.33it/s]\n",
      "100%|██████████| 268/268 [00:15<00:00, 17.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 1.0922293572853141, train_f1: 0.7022220846756022, train_auc: 0.9467752668313448, train_acc: 0.714901159180294, val_loss: 0.6782911906936274, val_f1: 0.8077753779697624, val_auc: 0.9916523396579271, val_acc: 0.8077753779697624\n",
      "----- Save model ----- with f1: 0.8077753779697624\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [03:00<00:00, 13.37it/s]\n",
      "100%|██████████| 268/268 [00:13<00:00, 19.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.7025283856243393, train_f1: 0.7991066406262641, train_auc: 0.9856910394435883, train_acc: 0.8058178948457876, val_loss: 0.6155361468806418, val_f1: 0.8277975599789855, val_auc: 0.9942351163035886, val_acc: 0.8277975599789855\n",
      "----- Save model ----- with f1: 0.8277975599789855\n",
      "Epoch 3/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [03:01<00:00, 13.32it/s]\n",
      "100%|██████████| 268/268 [00:13<00:00, 19.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.6232736231512489, train_f1: 0.8229531595483017, train_auc: 0.9920834468493352, train_acc: 0.827229093355413, val_loss: 0.5722406935989302, val_f1: 0.8369622322106124, val_auc: 0.9952148681748979, val_acc: 0.8369622322106124\n",
      "----- Save model ----- with f1: 0.8369622322106124\n",
      "Epoch 4/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [03:00<00:00, 13.38it/s]\n",
      "100%|██████████| 268/268 [00:13<00:00, 19.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.5731458326505616, train_f1: 0.8382820641664582, train_auc: 0.9944538155277927, train_acc: 0.8414924446284413, val_loss: 0.5331745474084988, val_f1: 0.8497460743681046, val_auc: 0.9958331477408117, val_acc: 0.8497460743681046\n",
      "----- Save model ----- with f1: 0.8497460743681046\n",
      "Epoch 5/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [03:00<00:00, 13.35it/s]\n",
      "100%|██████████| 268/268 [00:13<00:00, 19.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.5355925065779827, train_f1: 0.8494120224462562, train_auc: 0.9956135686107127, train_acc: 0.8518681432415649, val_loss: 0.510413982766444, val_f1: 0.8540073550872687, val_auc: 0.9967179008529059, val_acc: 0.8540073550872687\n",
      "----- Save model ----- with f1: 0.8540073550872687\n",
      "Epoch 6/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [02:57<00:00, 13.62it/s]\n",
      "100%|██████████| 268/268 [00:13<00:00, 19.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.5064111426873571, train_f1: 0.8567093603328522, train_auc: 0.9962740554936826, train_acc: 0.8588672117574001, val_loss: 0.5062863157311486, val_f1: 0.8567509193859086, val_auc: 0.9968160695898506, val_acc: 0.8567509193859086\n",
      "----- Save model ----- with f1: 0.8567509193859086\n",
      "Epoch 7/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [02:58<00:00, 13.51it/s]\n",
      "100%|██████████| 268/268 [00:13<00:00, 19.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.477148773839439, train_f1: 0.8659269726562532, train_auc: 0.9968693976633409, train_acc: 0.8677292486027738, val_loss: 0.5219735392411031, val_f1: 0.8472360049033915, val_auc: 0.9967372258923828, val_acc: 0.8472360049033915\n",
      "Epoch 8/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [02:59<00:00, 13.48it/s]\n",
      "100%|██████████| 268/268 [00:13<00:00, 19.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.4542539094617927, train_f1: 0.8714898184445289, train_auc: 0.9973935201509435, train_acc: 0.8730400020699648, val_loss: 0.4901175560123882, val_f1: 0.860370089311774, val_auc: 0.9971149015941828, val_acc: 0.860370089311774\n",
      "----- Save model ----- with f1: 0.860370089311774\n",
      "Epoch 9/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [02:57<00:00, 13.58it/s]\n",
      "100%|██████████| 268/268 [00:13<00:00, 19.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.4315868401529872, train_f1: 0.877582500066523, train_auc: 0.9977345740499567, train_acc: 0.8789070585800042, val_loss: 0.4889046659609721, val_f1: 0.8601949681863289, val_auc: 0.997200476996231, val_acc: 0.8601949681863289\n",
      "Epoch 10/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [02:58<00:00, 13.56it/s]\n",
      "100%|██████████| 268/268 [00:13<00:00, 19.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.41175998593002106, train_f1: 0.8834744909407742, train_auc: 0.9980090996818246, train_acc: 0.8846253363692818, val_loss: 0.495161481493556, val_f1: 0.860370089311774, val_auc: 0.9968884809488601, val_acc: 0.860370089311774\n",
      "Epoch 11/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [03:02<00:00, 13.25it/s]\n",
      "100%|██████████| 268/268 [00:15<00:00, 16.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.3918364151374791, train_f1: 0.8879851269950384, train_auc: 0.9983663285280903, train_acc: 0.8889593251914717, val_loss: 0.49484333926255814, val_f1: 0.8622380479831884, val_auc: 0.9970939269490398, val_acc: 0.8622380479831884\n",
      "----- Save model ----- with f1: 0.8622380479831884\n",
      "Epoch 12/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [03:01<00:00, 13.29it/s]\n",
      "100%|██████████| 268/268 [00:16<00:00, 16.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.37049453083536815, train_f1: 0.8949041362354461, train_auc: 0.9985393920065054, train_acc: 0.895744928586214, val_loss: 0.49685462462062485, val_f1: 0.8594944836845485, val_auc: 0.9968518754087962, val_acc: 0.8594944836845485\n",
      "Epoch 13/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [03:00<00:00, 13.37it/s]\n",
      "100%|██████████| 268/268 [00:15<00:00, 16.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.35472152153622055, train_f1: 0.8978215221214189, train_auc: 0.9987540282019, train_acc: 0.8986234734009522, val_loss: 0.501890771104885, val_f1: 0.8596112311015118, val_auc: 0.9968277618654467, val_acc: 0.8596112311015118\n",
      "Epoch 14/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [02:59<00:00, 13.49it/s]\n",
      "100%|██████████| 268/268 [00:15<00:00, 16.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.3383792034159132, train_f1: 0.9034511131458478, train_auc: 0.9988963601737206, train_acc: 0.9040765369488719, val_loss: 0.5067217576581593, val_f1: 0.8641060066546028, val_auc: 0.9968635657034383, val_acc: 0.8641060066546028\n",
      "----- Save model ----- with f1: 0.8641060066546028\n",
      "Epoch 15/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [02:59<00:00, 13.47it/s]\n",
      "100%|██████████| 268/268 [00:15<00:00, 17.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.3221863976414532, train_f1: 0.9070764934242027, train_auc: 0.9990762517388745, train_acc: 0.9076407576071207, val_loss: 0.5275501998892028, val_f1: 0.8532484968770065, val_auc: 0.9967836328534084, val_acc: 0.8532484968770065\n",
      "Epoch 16/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [02:59<00:00, 13.46it/s]\n",
      "100%|██████████| 268/268 [00:15<00:00, 16.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.3055100919607883, train_f1: 0.9116209023518621, train_auc: 0.9991790056892891, train_acc: 0.9121429310701719, val_loss: 0.5167063747935777, val_f1: 0.8629969061934505, val_auc: 0.9962848472745279, val_acc: 0.8629969061934505\n",
      "Epoch 17/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [02:59<00:00, 13.48it/s]\n",
      "100%|██████████| 268/268 [00:15<00:00, 17.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.2894845689594478, train_f1: 0.9156906391690208, train_auc: 0.9992793056492777, train_acc: 0.916108207410474, val_loss: 0.5353503776644707, val_f1: 0.8590274940166949, val_auc: 0.9962664979672214, val_acc: 0.8590274940166949\n",
      "Epoch 18/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [03:01<00:00, 13.34it/s]\n",
      "100%|██████████| 268/268 [00:13<00:00, 19.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.27338492716292495, train_f1: 0.9208435658684702, train_auc: 0.9993973955588118, train_acc: 0.9211731525564065, val_loss: 0.5297433064777906, val_f1: 0.8638141381121943, val_auc: 0.9964816507916657, val_acc: 0.8638141381121943\n",
      "Epoch 19/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [03:01<00:00, 13.30it/s]\n",
      "100%|██████████| 268/268 [00:13<00:00, 19.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.25827259787000617, train_f1: 0.9249354460831131, train_auc: 0.9994575921457372, train_acc: 0.9252031152970399, val_loss: 0.5444442749704554, val_f1: 0.8617710583153347, val_auc: 0.9962460751765844, val_acc: 0.8617710583153347\n",
      "Epoch 20/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [03:00<00:00, 13.40it/s]\n",
      "100%|██████████| 268/268 [00:13<00:00, 19.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.24607926318483633, train_f1: 0.9274907524336157, train_auc: 0.9995399063818905, train_acc: 0.9277452908300559, val_loss: 0.5761197935094808, val_f1: 0.8527231335006713, val_auc: 0.9962584743901981, val_acc: 0.8527231335006713\n",
      "Epoch 21/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [02:58<00:00, 13.53it/s]\n",
      "100%|██████████| 268/268 [00:13<00:00, 19.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.22961483202529268, train_f1: 0.9322635909560975, train_auc: 0.9996132427505188, train_acc: 0.9324609294142, val_loss: 0.5686823093374054, val_f1: 0.857451403887689, val_auc: 0.9963010206974563, val_acc: 0.857451403887689\n",
      "Epoch 22/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [02:58<00:00, 13.55it/s]\n",
      "100%|██████████| 268/268 [00:13<00:00, 19.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.2187577329625358, train_f1: 0.9349223087188451, train_auc: 0.9996647005769455, train_acc: 0.9350936659076796, val_loss: 0.5812323602931154, val_f1: 0.8499211954935497, val_auc: 0.9958960261137226, val_acc: 0.8499211954935497\n",
      "Epoch 23/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [02:53<00:00, 13.90it/s]\n",
      "100%|██████████| 268/268 [00:13<00:00, 19.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.20622484969894148, train_f1: 0.9385206950793149, train_auc: 0.9997115414085347, train_acc: 0.938690229766094, val_loss: 0.5877468769174462, val_f1: 0.8581518883894694, val_auc: 0.9962417516061545, val_acc: 0.8581518883894694\n",
      "Epoch 24/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [02:56<00:00, 13.67it/s]\n",
      "100%|██████████| 268/268 [00:13<00:00, 19.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.1951768040049522, train_f1: 0.9415828761006672, train_auc: 0.9997475640586502, train_acc: 0.9417175533015939, val_loss: 0.6179557618365351, val_f1: 0.8490455898663242, val_auc: 0.9959890882681232, val_acc: 0.8490455898663242\n",
      "Epoch 25/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [02:57<00:00, 13.60it/s]\n",
      "100%|██████████| 268/268 [00:13<00:00, 19.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.18466397895499284, train_f1: 0.9445281266863541, train_auc: 0.9997729647262389, train_acc: 0.9446284413164976, val_loss: 0.6218775996674243, val_f1: 0.851614033039519, val_auc: 0.9959354936749367, val_acc: 0.851614033039519\n",
      "Epoch 26/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [02:57<00:00, 13.59it/s]\n",
      "100%|██████████| 268/268 [00:13<00:00, 19.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.17511159152066982, train_f1: 0.9466679877728259, train_auc: 0.9998025025850651, train_acc: 0.9467695611674601, val_loss: 0.6366484570386473, val_f1: 0.8520810227073726, val_auc: 0.9959237672451825, val_acc: 0.8520810227073726\n",
      "Epoch 27/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [02:54<00:00, 13.84it/s]\n",
      "100%|██████████| 268/268 [00:13<00:00, 19.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.16516223701915134, train_f1: 0.94898891190673, train_auc: 0.999830535711167, train_acc: 0.9490723970192507, val_loss: 0.6428541237516189, val_f1: 0.8567509193859086, val_auc: 0.9957752668833894, val_acc: 0.8567509193859086\n",
      "Epoch 28/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [02:53<00:00, 13.90it/s]\n",
      "100%|██████████| 268/268 [00:13<00:00, 19.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.15658065229530907, train_f1: 0.9520211711359359, train_auc: 0.9998559094907327, train_acc: 0.9520803146346513, val_loss: 0.6473996080041154, val_f1: 0.8540657287957504, val_auc: 0.9958372175987152, val_acc: 0.8540657287957504\n",
      "Epoch 29/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [02:53<00:00, 13.90it/s]\n",
      "100%|██████████| 268/268 [00:13<00:00, 19.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.14764000437282548, train_f1: 0.9545307895881741, train_auc: 0.9998630164283239, train_acc: 0.9545901469675016, val_loss: 0.7203726787050605, val_f1: 0.8466522678185745, val_auc: 0.9952422146848514, val_acc: 0.8466522678185745\n",
      "Epoch 30/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [02:55<00:00, 13.78it/s]\n",
      "100%|██████████| 268/268 [00:13<00:00, 19.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.13979492432727714, train_f1: 0.9563583696188133, train_auc: 0.9998928247647824, train_acc: 0.9564143034568412, val_loss: 0.7137164866774163, val_f1: 0.8479364894051719, val_auc: 0.9954369422614489, val_acc: 0.8479364894051719\n"
     ]
    }
   ],
   "source": [
    "# 这段代码初始化了GAT分类器模型，并调用了训练函数train来在训练集和验证集上进行模型训练。\n",
    "\n",
    "# 1、确定类别数量：\n",
    "num_classes = len(label2idx)\n",
    "\n",
    "# 2、模型初始化：\n",
    "model = GATClassifier(args.embedding_dim, args.hidden_dim, args.num_heads, num_classes)\n",
    "\n",
    "# 3、打印训练信息：\n",
    "print('-------- GAT 模型训练')\n",
    "\n",
    "# 4、调用训练函数\n",
    "train(args, model,                                                 \n",
    "      (train_x_adj, train_x_feature, train_df['target'].values),   \n",
    "      (valid_x_adj, valid_x_feature, valid_df['target'].values),   \n",
    "      num_classes, \n",
    "      \"GAT\")                                          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCN 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- GCN 模型训练\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [02:19<00:00, 17.35it/s]\n",
      "100%|██████████| 268/268 [00:12<00:00, 21.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 1.70563102121286, train_f1: 0.5420636602951951, train_auc: 0.9083831769678864, train_acc: 0.56693748706272, val_loss: 1.0791760883224542, val_f1: 0.692720798552332, val_auc: 0.9769426481044641, val_acc: 0.692720798552332\n",
      "----- Save model ----- with f1: 0.692720798552332\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [02:19<00:00, 17.30it/s]\n",
      "100%|██████████| 268/268 [00:12<00:00, 21.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 1.0208729879424858, train_f1: 0.6973118519205526, train_auc: 0.9750637048755078, train_acc: 0.7120226143655558, val_loss: 0.8834435901701899, val_f1: 0.7512112544509952, val_auc: 0.9872050748319297, val_acc: 0.7512112544509952\n",
      "----- Save model ----- with f1: 0.7512112544509952\n",
      "Epoch 3/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [02:19<00:00, 17.34it/s]\n",
      "100%|██████████| 268/268 [00:12<00:00, 21.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.9049675517672344, train_f1: 0.732694591797364, train_auc: 0.9842084829634251, train_acc: 0.7437254191678742, val_loss: 0.8279148435859542, val_f1: 0.759033331387543, val_auc: 0.9900965561456779, val_acc: 0.759033331387543\n",
      "----- Save model ----- with f1: 0.759033331387543\n",
      "Epoch 4/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [02:19<00:00, 17.36it/s]\n",
      "100%|██████████| 268/268 [00:12<00:00, 21.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.8413294656880629, train_f1: 0.7514693443824094, train_auc: 0.9881859507991987, train_acc: 0.7608737838956737, val_loss: 0.7791434359639438, val_f1: 0.7707664467923647, val_auc: 0.9914078307577585, val_acc: 0.7707664467923647\n",
      "----- Save model ----- with f1: 0.7707664467923647\n",
      "Epoch 5/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [02:19<00:00, 17.30it/s]\n",
      "100%|██████████| 268/268 [00:12<00:00, 21.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.797553832747587, train_f1: 0.7653928319017695, train_auc: 0.9899467817897807, train_acc: 0.7733582591595943, val_loss: 0.7350120159950276, val_f1: 0.7848345105364544, val_auc: 0.9928442071434231, val_acc: 0.7848345105364544\n",
      "----- Save model ----- with f1: 0.7848345105364544\n",
      "Epoch 6/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [02:19<00:00, 17.37it/s]\n",
      "100%|██████████| 268/268 [00:12<00:00, 21.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.7645369635413813, train_f1: 0.7758688796020065, train_auc: 0.9913878466713875, train_acc: 0.782931846408611, val_loss: 0.7203054187624756, val_f1: 0.790555133967661, val_auc: 0.9932542075390813, val_acc: 0.790555133967661\n",
      "----- Save model ----- with f1: 0.790555133967661\n",
      "Epoch 7/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [02:19<00:00, 17.37it/s]\n",
      "100%|██████████| 268/268 [00:12<00:00, 21.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.738159859417291, train_f1: 0.7846174459893845, train_auc: 0.9921229911229648, train_acc: 0.7911211446905403, val_loss: 0.6985126103538631, val_f1: 0.7912556184694414, val_auc: 0.9937670633466157, val_acc: 0.7912556184694414\n",
      "----- Save model ----- with f1: 0.7912556184694414\n",
      "Epoch 8/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [02:19<00:00, 17.37it/s]\n",
      "100%|██████████| 268/268 [00:12<00:00, 21.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.7151559111569705, train_f1: 0.7904812513831898, train_auc: 0.9927982642341476, train_acc: 0.796412492237632, val_loss: 0.6771901762946997, val_f1: 0.8017628859961473, val_auc: 0.9939101852977433, val_acc: 0.8017628859961473\n",
      "----- Save model ----- with f1: 0.8017628859961473\n",
      "Epoch 9/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [02:19<00:00, 17.34it/s]\n",
      "100%|██████████| 268/268 [00:12<00:00, 21.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.696391148949105, train_f1: 0.7974773997845197, train_auc: 0.9933029875389844, train_acc: 0.8028746636307182, val_loss: 0.6507026108621218, val_f1: 0.808826104722433, val_auc: 0.994303560904337, val_acc: 0.808826104722433\n",
      "----- Save model ----- with f1: 0.808826104722433\n",
      "Epoch 10/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [02:19<00:00, 17.36it/s]\n",
      "100%|██████████| 268/268 [00:12<00:00, 21.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.6794004837507444, train_f1: 0.8025227279903628, train_auc: 0.9936346081809512, train_acc: 0.8074673980542331, val_loss: 0.6503346029724649, val_f1: 0.8101687000175121, val_auc: 0.9944705025520562, val_acc: 0.8101687000175121\n",
      "----- Save model ----- with f1: 0.8101687000175121\n",
      "Epoch 11/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [02:19<00:00, 17.36it/s]\n",
      "100%|██████████| 268/268 [00:12<00:00, 21.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.6649569172523185, train_f1: 0.8066214684592167, train_auc: 0.9940468184403971, train_acc: 0.8111739287932105, val_loss: 0.6473361017432674, val_f1: 0.8137294962348958, val_auc: 0.9947674390484095, val_acc: 0.8137294962348958\n",
      "----- Save model ----- with f1: 0.8137294962348958\n",
      "Epoch 12/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [02:18<00:00, 17.39it/s]\n",
      "100%|██████████| 268/268 [00:12<00:00, 21.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.6507172159147466, train_f1: 0.8111647404283266, train_auc: 0.9943555805743409, train_acc: 0.8154367625750363, val_loss: 0.6224231384972582, val_f1: 0.818749635164322, val_auc: 0.9945730961709727, val_acc: 0.818749635164322\n",
      "----- Save model ----- with f1: 0.818749635164322\n",
      "Epoch 13/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [02:19<00:00, 17.34it/s]\n",
      "100%|██████████| 268/268 [00:12<00:00, 21.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.6402212956562543, train_f1: 0.8142884356768689, train_auc: 0.994581634515436, train_acc: 0.8183735251500724, val_loss: 0.621001297649719, val_f1: 0.816998423909871, val_auc: 0.9949076959657297, val_acc: 0.816998423909871\n",
      "Epoch 14/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [02:18<00:00, 17.41it/s]\n",
      "100%|██████████| 268/268 [00:12<00:00, 21.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.6289891175222618, train_f1: 0.8181693742271351, train_auc: 0.9949795032651285, train_acc: 0.8219894949285862, val_loss: 0.618053205909013, val_f1: 0.8211429572120716, val_auc: 0.994856360474898, val_acc: 0.8211429572120716\n",
      "----- Save model ----- with f1: 0.8211429572120716\n",
      "Epoch 15/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [02:19<00:00, 17.32it/s]\n",
      "100%|██████████| 268/268 [00:12<00:00, 21.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.617848438759709, train_f1: 0.8207619900324162, train_auc: 0.9952336215318419, train_acc: 0.8243311426205755, val_loss: 0.5996244533348886, val_f1: 0.8236530266767847, val_auc: 0.9950411609245347, val_acc: 0.8236530266767847\n",
      "----- Save model ----- with f1: 0.8236530266767847\n",
      "Epoch 16/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [02:18<00:00, 17.42it/s]\n",
      "100%|██████████| 268/268 [00:12<00:00, 21.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.6074996339218889, train_f1: 0.824261687470014, train_auc: 0.9953844746016522, train_acc: 0.827662492237632, val_loss: 0.6052595321506038, val_f1: 0.826338217266943, val_auc: 0.9950232564065279, val_acc: 0.826338217266943\n",
      "----- Save model ----- with f1: 0.826338217266943\n",
      "Epoch 17/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [02:18<00:00, 17.49it/s]\n",
      "100%|██████████| 268/268 [00:12<00:00, 21.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.6003346379032192, train_f1: 0.8265833445057952, train_auc: 0.9956027756426878, train_acc: 0.8298553612088595, val_loss: 0.6051640101611162, val_f1: 0.8219018154223338, val_auc: 0.9950875999828106, val_acc: 0.8219018154223338\n",
      "Epoch 18/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [02:17<00:00, 17.57it/s]\n",
      "100%|██████████| 268/268 [00:12<00:00, 21.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.5914679703777593, train_f1: 0.829020735899134, train_auc: 0.9958533465642432, train_acc: 0.8321323225005175, val_loss: 0.6113076831089025, val_f1: 0.8212597046290351, val_auc: 0.9956156413014681, val_acc: 0.8212597046290351\n",
      "Epoch 19/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [02:17<00:00, 17.63it/s]\n",
      "100%|██████████| 268/268 [00:12<00:00, 21.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.5844775183521066, train_f1: 0.8308937325080236, train_auc: 0.9959465035889681, train_acc: 0.8339500103498241, val_loss: 0.5916018176392944, val_f1: 0.8279726811044306, val_auc: 0.9954429762025574, val_acc: 0.8279726811044306\n",
      "----- Save model ----- with f1: 0.8279726811044306\n",
      "Epoch 20/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [02:15<00:00, 17.83it/s]\n",
      "100%|██████████| 268/268 [00:14<00:00, 18.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.5758573040674551, train_f1: 0.833591104997876, train_auc: 0.9961718420849038, train_acc: 0.8364533740426413, val_loss: 0.5840155575146425, val_f1: 0.8287899130231744, val_auc: 0.9953227749729603, val_acc: 0.8287899130231744\n",
      "----- Save model ----- with f1: 0.8287899130231744\n",
      "Epoch 21/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [02:15<00:00, 17.87it/s]\n",
      "100%|██████████| 268/268 [00:14<00:00, 18.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.5704981572679341, train_f1: 0.8354917013605583, train_auc: 0.9962224173111548, train_acc: 0.8382387186917822, val_loss: 0.6192898471742424, val_f1: 0.8214931994629618, val_auc: 0.995683829370567, val_acc: 0.8214931994629618\n",
      "Epoch 22/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [02:17<00:00, 17.60it/s]\n",
      "100%|██████████| 268/268 [00:12<00:00, 21.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.5640172880414319, train_f1: 0.8372597403839597, train_auc: 0.9964034420351947, train_acc: 0.8399464396605257, val_loss: 0.5809881692724443, val_f1: 0.8298990134843267, val_auc: 0.9955581299601789, val_acc: 0.8298990134843267\n",
      "----- Save model ----- with f1: 0.8298990134843267\n",
      "Epoch 23/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [02:17<00:00, 17.55it/s]\n",
      "100%|██████████| 268/268 [00:12<00:00, 21.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.5573606120656845, train_f1: 0.8393272440567713, train_auc: 0.9965418638240968, train_acc: 0.8418999689505279, val_loss: 0.5631213314803458, val_f1: 0.8372541007530209, val_auc: 0.9957769029294663, val_acc: 0.8372541007530209\n",
      "----- Save model ----- with f1: 0.8372541007530209\n",
      "Epoch 24/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [02:17<00:00, 17.61it/s]\n",
      "100%|██████████| 268/268 [00:12<00:00, 21.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.5509481588563581, train_f1: 0.8406282453258962, train_auc: 0.9967002244359694, train_acc: 0.8431484164769198, val_loss: 0.570920157538199, val_f1: 0.835094273539198, val_auc: 0.995950219254646, val_acc: 0.835094273539198\n",
      "Epoch 25/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [02:14<00:00, 17.90it/s]\n",
      "100%|██████████| 268/268 [00:14<00:00, 18.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.5473425724722496, train_f1: 0.8413587049944453, train_auc: 0.9967770819915275, train_acc: 0.8437758745601325, val_loss: 0.5688616361524631, val_f1: 0.8371957270445392, val_auc: 0.9957936364788128, val_acc: 0.8371957270445392\n",
      "Epoch 26/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [02:15<00:00, 17.81it/s]\n",
      "100%|██████████| 268/268 [00:14<00:00, 18.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.5409764412186101, train_f1: 0.8436776230830729, train_auc: 0.9968774798940186, train_acc: 0.8460334299316912, val_loss: 0.5676428061604167, val_f1: 0.8332263148677835, val_auc: 0.9958065187952272, val_acc: 0.8332263148677835\n",
      "Epoch 27/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [02:14<00:00, 17.99it/s]\n",
      "100%|██████████| 268/268 [00:14<00:00, 18.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.5356693285400198, train_f1: 0.8453024032307087, train_auc: 0.9969200313227096, train_acc: 0.8474824052991099, val_loss: 0.5633477215279845, val_f1: 0.837020605919094, val_auc: 0.9957996163651521, val_acc: 0.837020605919094\n",
      "Epoch 28/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [02:14<00:00, 17.95it/s]\n",
      "100%|██████████| 268/268 [00:14<00:00, 19.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.5315739883554002, train_f1: 0.8461081819972337, train_auc: 0.9970663650796853, train_acc: 0.8482715793831505, val_loss: 0.5658971046639693, val_f1: 0.8349191524137528, val_auc: 0.9958888201925733, val_acc: 0.8349191524137528\n",
      "Epoch 29/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [02:14<00:00, 18.00it/s]\n",
      "100%|██████████| 268/268 [00:12<00:00, 21.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.5259153211662021, train_f1: 0.847874047809756, train_auc: 0.9971989633679982, train_acc: 0.8500245808321258, val_loss: 0.5664621611584479, val_f1: 0.8371957270445392, val_auc: 0.9959073274981556, val_acc: 0.8371957270445392\n",
      "Epoch 30/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2416/2416 [02:14<00:00, 17.99it/s]\n",
      "100%|██████████| 268/268 [00:12<00:00, 21.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.5214620593269136, train_f1: 0.8494699363666118, train_auc: 0.9971907440039245, train_acc: 0.851551179879942, val_loss: 0.5755264366796217, val_f1: 0.8336349308271555, val_auc: 0.9959132521428402, val_acc: 0.8336349308271555\n"
     ]
    }
   ],
   "source": [
    "# 这段代码初始化了GCN分类器模型，并调用了训练函数train来进行模型训练。\n",
    "\n",
    "# 1、确定类别数量：\n",
    "num_classes = len(label2idx)\n",
    "\n",
    "# 2、模型初始化：\n",
    "model = GCNClassifier(args.embedding_dim, args.hidden_dim, num_classes)\n",
    "\n",
    "# 3、打印训练信息：\n",
    "print('-------- GCN 模型训练')\n",
    "\n",
    "# 4、调用训练函数：\n",
    "train(args, model,                                                 \n",
    "      (train_x_adj, train_x_feature, train_df['target'].values),   \n",
    "      (valid_x_adj, valid_x_feature, valid_df['target'].values),   \n",
    "      num_classes,                                                 \n",
    "      \"GCN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这段代码定义了一个load_model函数，用于加载预训练的模型权重，并将模型设置为评估模式\n",
    "def load_model(model, best_model_path):\n",
    "    \"\"\"加载模型、加载模型权重\"\"\"\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "    # 调用model.eval()将模型设置为评估模式。在评估模式下，模型的一些行为会有所不同，\n",
    "    model.eval()\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这段代码定义了test函数，用于在测试集上评估模型的性能。\n",
    "def test(args, model, x_adj, x_feature, label_list, num_classes):\n",
    "    \"\"\"测试\"\"\"\n",
    "# 1、创建测试数据集和数据加载器\n",
    "    testdataset = GraphDataset(x_adj, x_feature)\n",
    "    testloader = GraphDataLoader(testdataset, batch_size = args.batch_size, shuffle = False)\n",
    "\n",
    "    # 2、模型预测\n",
    "    pred_list = []\n",
    "    with torch.no_grad():                             \n",
    "        for idx, G in enumerate(tqdm(testloader)):    \n",
    "            h = G.ndata['feat'].float()              \n",
    "            log = model(G, h)                         \n",
    "            logits = log.softmax(-1)                  \n",
    "            pred_soft = logits.detach().cpu().numpy() \n",
    "            pred_list.append(np.argmax(pred_soft, 1)) \n",
    "\n",
    "        preds = np.concatenate(pred_list)             \n",
    "\n",
    "    # 计算加权平均F1分数、精确度、召回率和准确率，使用f1_score、precision_score、recall_score和accuracy_score函数，\n",
    "    # 其中average='weighted'参数表示计算加权平均值，权重由每个类别的样本数量决定\n",
    "    print('F1 in test:', f1_score(label_list, preds, average = 'weighted'))\n",
    "    print('precision in test:', precision_score(label_list, preds, average = 'weighted'))\n",
    "    print('recall in test:', recall_score(label_list, preds, average = 'weighted'))\n",
    "    print('accuracy in test:', accuracy_score(label_list, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 508/508 [00:26<00:00, 18.85it/s]\n",
      "/home/sunmr/anaconda3/envs/sun/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 in test: 0.828065190694157\n",
      "precision in test: 0.8405425486020889\n",
      "recall in test: 0.834963701242771\n",
      "accuracy in test: 0.834963701242771\n"
     ]
    }
   ],
   "source": [
    "# 1、确定类别数量：num_classes变量通过计算label2idx字典的长度来确定类别数量。 \n",
    "num_classes = len(label2idx)\n",
    "\n",
    "# 2、模型初始化和加载\n",
    "model = GATClassifier(args.embedding_dim, args.hidden_dim, args.num_heads, num_classes)\n",
    "\n",
    "# 定义best_model_path变量，指向保存最佳模型权重的文件路径。\n",
    "best_model_path = f'{args.model_save_path}/GAT-epoch-13-0.8641060066546028.pt'\n",
    "\n",
    "# 调用load_model函数，加载best_model_path中的模型权重到model实例上，并将模型设置为评估模式\n",
    "model = load_model(model, best_model_path)\n",
    "\n",
    "# 调用test函数，传入参数args（包含训练配置）、model（加载了最佳权重的GAT分类器模型）、测试集的邻接矩阵test_x_adj、\n",
    "test(args, model, test_x_adj, test_x_feature, test_df['target'].values, len(label2idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 508/508 [00:24<00:00, 20.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 in test: 0.7988395395765523\n",
      "precision in test: 0.8130721153032072\n",
      "recall in test: 0.8079242032730405\n",
      "accuracy in test: 0.8079242032730405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/sunmr/anaconda3/envs/sun/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# 1、模型初始化：创建GCNClassifier实例，传入参数args.embedding_dim（输入特征维度）、args.hidden_dim（隐藏层维度）和num_classes（类别数量）。\n",
    "model = GCNClassifier(args.embedding_dim, args.hidden_dim, num_classes)\n",
    "\n",
    "# 2、加载最佳模型权重\n",
    "best_model_path = f'{args.model_save_path}/GCN-epoch-22-0.8372541007530209.pt'\n",
    "model = load_model(model, best_model_path)\n",
    "\n",
    "# 3、用test函数，传入参数args（包含训练配置）、model（加载了最佳权重的GCN分类器模型）、测试集的邻接矩阵test_x_adj、\n",
    "test(args, model, test_x_adj, test_x_feature, test_df['target'].values, len(label2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sun",
   "language": "python",
   "name": "sun"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
